{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inter Annotation Agreement Fleiss Kappa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimashiRathnayake/CMCS-Text-Classification/blob/main/Dataset_Evaluation/Inter_Annotation_Agreement_Fleiss_Kappa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEObw5biNZiv"
      },
      "source": [
        "**Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW1JyIKmNX4g"
      },
      "source": [
        "tag_set = \"HateSpeech\" #@param [\"Sentiment\",\"Humor\",\"Aspect\",\"HateSpeech\",\"LanguageID\"]\n",
        "sentences_no = 156 #{allow-input: true} #156, 1349\n",
        "annotators = ['janani','raveesha','himashi']\n",
        "\n",
        "if (tag_set==\"Sentiment\"):\n",
        "  tags = ['Positive', 'Negative','Neutral','Conflict'];\n",
        "elif (tag_set==\"Humor\"):\n",
        "  tags = ['Humorous','Non-humorous'];\n",
        "elif (tag_set==\"HateSpeech\"):\n",
        " tags = ['Hate-Inducing', 'Abusive', 'Not offensive'];\n",
        "elif (tag_set==\"Aspect\"):\n",
        "  tags = ['Billing or price','Customer service','Data','Network','None','Package','Service or product'];\n",
        "elif (tag_set==\"LanguageID\"):\n",
        "  tags = ['Sinhala','English','Sin-Eng','Eng-Sin','Unknown','name','Mixed'];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voMiSnkGOyiB"
      },
      "source": [
        "# !pip install dkpro-cassis > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD2Qbjn7jQ2a",
        "outputId": "b34a17d5-c203-4c54-c1a7-51ddfa8f9e8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96GBn6fQqvyG"
      },
      "source": [
        "#### **Generate Input Matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H854ncZRqWF2"
      },
      "source": [
        "##### **Read UIMA CAS XMI File to a matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bopkYo1wZops"
      },
      "source": [
        "# from cassis import *\n",
        "# import numpy as np\n",
        "\n",
        "# tags_no = len(tags)\n",
        "# matrix_to_kappa = np.zeros((sentences_no, tags_no), dtype=int)\n",
        "# compare_matrix = np.zeros((sentences_no, 3), dtype=int)\n",
        "\n",
        "# for annotator in annotators:\n",
        "\n",
        "#   with open('/content/drive/Shareddrives/FYP-CodeStars/Annotations/'+annotator+'/TypeSystem.xml', 'rb') as f:\n",
        "#     typesystem = load_typesystem(f)\n",
        "\n",
        "#   with open('/content/drive/Shareddrives/FYP-CodeStars/Annotations/'+annotator+'/Full_Kappa_Annotation_'+annotator+'.xmi', 'rb') as f:\n",
        "#     doc = load_cas_from_xmi(f, typesystem=typesystem)\n",
        "\n",
        "#   count = 0;\n",
        "#   for (index, sentence) in enumerate(doc.select('webanno.custom.'+tag_set)):\n",
        "#     # print(f\"{sentence.get_covered_text()}\\t{sentence.Aspect}\")\n",
        "#     for (tag_id, tag) in enumerate(tags):\n",
        "#       if (sentence.Humor==tag): #Change to tag name\n",
        "#         # print(f\"{sentence.get_covered_text()}\\t{sentence.HateSpeech}\")\n",
        "#         matrix_to_kappa[index,tag_id]=matrix_to_kappa[index,tag_id]+1\n",
        "    \n",
        "#   # print(count)\n",
        "# # print(matrix_to_kappa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCI5AhEuqo2L"
      },
      "source": [
        "##### **Read CSV file to a matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI43s_BEj4UB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "tags_no = len(tags)\n",
        "matrix_to_kappa = np.zeros((sentences_no, tags_no), dtype=int)\n",
        "compare_matrix = np.zeros((sentences_no, 3), dtype=int)\n",
        "\n",
        "all_data = pd.read_csv(\"/content/drive/Shareddrives/FYP-CodeStars/Annotations/Final/Kappa/Hate_Speech_Kappa.csv\")\n",
        "\n",
        "for annotator in annotators:\n",
        "\n",
        "  count = 0;\n",
        "  for (index, annotation) in enumerate(all_data[annotator]):\n",
        "    for (tag_id, tag) in enumerate(tags):\n",
        "      if (annotation==tag): #Change to tag name\n",
        "        # print(annotation, tag)\n",
        "        matrix_to_kappa[index,tag_id]=matrix_to_kappa[index,tag_id]+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeqfHiLqHg4B"
      },
      "source": [
        "## **Calculate Inter Annotation Agreement**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYc8hcS9RzT8"
      },
      "source": [
        "#### **Calculate Cohen Kappa** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEewNuMHGKEM"
      },
      "source": [
        "# import sklearn.metrics\n",
        "# sklearn.metrics.cohen_kappa_score(['humorous','non-humorous','humorous'], ['humorous','non-humorous',''], labels=None, weights=None, sample_weight=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yZJGY4mIJDm"
      },
      "source": [
        "#### **Calculate Fleiss Kappa**\n",
        "\n",
        "Reference: https://towardsdatascience.com/inter-annotator-agreement-2f46c6d37bf3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDyIzlWvGMep",
        "outputId": "dd7f387c-3f91-4c42-c875-22e6577210dd"
      },
      "source": [
        "def fleiss_kappa(M):\n",
        "    \"\"\"Computes Fleiss' kappa for group of annotators.\n",
        "    :param M: a matrix of shape (:attr:'N', :attr:'k') with 'N' = number of subjects and 'k' = the number of categories.\n",
        "        'M[i, j]' represent the number of raters who assigned the 'i'th subject to the 'j'th category.\n",
        "    :type: numpy matrix\n",
        "    :rtype: float\n",
        "    :return: Fleiss' kappa score\n",
        "    \"\"\"\n",
        "    N, k = M.shape  # N is # of items, k is # of categories\n",
        "    n_annotators = float(np.sum(M[0, :]))  # # of annotators\n",
        "    tot_annotations = N * n_annotators  # the total # of annotations\n",
        "    category_sum = np.sum(M, axis=0)  # the sum of each category over all items\n",
        "\n",
        "    # chance agreement\n",
        "    p = category_sum / tot_annotations  # the distribution of each category over all annotations\n",
        "    PbarE = np.sum(p * p)  # average chance agreement over all categories\n",
        "\n",
        "    # observed agreement\n",
        "    P = (np.sum(M * M, axis=1) - n_annotators) / (n_annotators * (n_annotators - 1))\n",
        "    Pbar = np.sum(P) / N  # add all observed agreement chances per item and divide by amount of items\n",
        "\n",
        "    return round((Pbar - PbarE) / (1 - PbarE), 4)\n",
        "\n",
        "fleiss_kappa(matrix_to_kappa)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8362"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyVpFROeI5JJ",
        "outputId": "7932e1a5-9091-4150-8552-5cb10e94e8b5"
      },
      "source": [
        "import statsmodels\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "fleiss_kappa(np.array(matrix_to_kappa))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8361754263139464"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B43F77Om9XUm"
      },
      "source": [
        "**Read annotations to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTuP-zlzMKjn",
        "outputId": "5481367b-7400-4475-f6f9-38d43eca3636"
      },
      "source": [
        "from cassis import *\n",
        "import numpy as np\n",
        "\n",
        "with open('/content/drive/Shareddrives/FYP-CodeStars/Annotations/Janani/TypeSystem.xml', 'rb') as f:\n",
        "  typesystem = load_typesystem(f)\n",
        "\n",
        "with open('/content/drive/Shareddrives/FYP-CodeStars/Annotations/Janani/Full_Kappa_Annotation_Janani.xmi', 'rb') as f1:\n",
        "  doc1 = load_cas_from_xmi(f1, typesystem=typesystem)\n",
        "\n",
        "count = 0;\n",
        "\n",
        "with open('/content/drive/Shareddrives/FYP/corpus/sentences_Humor_Set_2.csv', 'w') as f:\n",
        "\n",
        "  for sentence in doc1.select('webanno.custom.HateSpeech'):\n",
        "    \n",
        "    count+=1;\n",
        "    print(f\"{sentence.get_covered_text()}\\t{sentence.HateSpeech}\");\n",
        "\n",
        "print(count);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nisal Vimukthi මේක බල⁣ාං\tNot offensive\n",
            "Internet package😍😍\tNot offensive\n",
            "කොහොමත් එහෙමනෙ.\tNot offensive\n",
            "HI SLT,\tNot offensive\n",
            "Meva balannadeyakna\tNot offensive\n",
            "Good luck 👌\tNot offensive\n",
            "Dialog axiata app එක බාගත්තට තාම free data නෑ.\tNot offensive\n",
            "Chocolate\tNot offensive\n",
            "Pls introduce gaming mode to hbb routers\tNot offensive\n",
            "Ane....hodai\tNot offensive\n",
            "Debit card payment valata nadda?mobitel selfcare app eke?\tNot offensive\n",
            "Maradona the best player\tNot offensive\n",
            "Chaterma service denna network eka.\tAbusive\n",
            "eka vadakata 8 parak katha karanna one una karaganna.\tNot offensive\n",
            "thu nidaking jarama company ekak\tAbusive\n",
            "Budusaraniani\tNot offensive\n",
            "තාම තියද\tNot offensive\n",
            "card eka damme nane oyala\tNot offensive\n",
            "Thank you. 👌💗💗💗💗💗💗\tNot offensive\n",
            "මන් ගොඩක් කල් ඉදන්ම.HUTCH ,ETSALAT CLQ APP එක පාවිච්චි කරා...\tNot offensive\n",
            "දැන් 4G කියල ..3G SPEED අඩුම කරල ...ඇයි මේ...\tNot offensive\n",
            "4G දාගන්න කෝ.සල්ලි 😥😥😥😥😥T\tNot offensive\n",
            "Like curfew ends u guys give it great\tNot offensive\n",
            "Rest kare naththan moko wenne ???\tNot offensive\n",
            "He He =D\tNot offensive\n",
            "YouTube wada neee ☹️\tNot offensive\n",
            "I have recharge Rs. 500 to my Etisalat No 0720474556 on 19.04.2020 on 08.04 am via commercial bank online facility, the amount is debited and deducted from my account, but so far, i did not get any message from Hutch and my number did not recharge.\tNot offensive\n",
            "I wish you a very happy birthday sir.\tNot offensive\n",
            "Mullaitivu\tNot offensive\n",
            "DTV account number එවන්නෙ මොකටද උබලට.\tAbusive\n",
            "dialog දැන් චාටර් වෙලා ඉවරයි.\tNot offensive\n",
            "උබල බද්ද අපෙන් අය කරනෙක නවත්වනම් වෙන්නෙ උබලගෙ පරිහානියම විතරයි දැන්.\tAbusive\n",
            "dialog tv වලට හොද තරගකරුවෙක් ආපු ගමන් උබල වැටෙනවා සනිකව\tNot offensive\n",
            "kohomada\tNot offensive\n",
            "When i send the invites to dialog numbers. It's says \"free data offer is exclusively available for dialog numbers only\" ??\tNot offensive\n",
            "What can i do\tNot offensive\n",
            "Hey, SLT... it's me again.\tNot offensive\n",
            "the one who will put slt 4G router on fire as soon as got another ISP in area.\tNot offensive\n",
            "Today specials * No Internet till morning since yesterday night.\tNot offensive\n",
            "* Connection drops every 30 minutes for about 15 min and restore again since morning.\tNot offensive\n",
            "FYI: the freaking tech guy who promised by two of your customer care center representatives still not came.\tNot offensive\n",
            "(promised to come weeks ago)\tNot offensive\n",
            "FYI 2: You suck\tNot offensive\n",
            "FYI 3: The social media guy(s) don't ask me to PM my connection number.\tNot offensive\n",
            "check your chat history, you'll find plenty of complains made by me in the past.\tNot offensive\n",
            "Shame !!! shame !!!! shame !!!\tNot offensive\n",
            "All the best Lionz\tNot offensive\n",
            "My 4G router is not working properly.\tNot offensive\n",
            "Already I have complained 1212 and sent messenges to the inbox too.\tNot offensive\n",
            "Please fix my 4G router as soon as possible.\tNot offensive\n",
            "තෙරුවන් සරණයි\tNot offensive\n",
            "happy birthday sanga aiya,have a great day..\tNot offensive\n",
            "Tnx!\tNot offensive\n",
            "Today's votes are in! Dial *646# to predict the winning team every day!\tNot offensive\n",
            "ape paththata 2G wath ne\tNot offensive\n",
            "Prepaid router price please\tNot offensive\n",
            "12 pro mini price\tNot offensive\n",
            "Done..\tNot offensive\n",
            "හම්බ උනා ලබිබක් 😡\tAbusive\n",
            "❤❤❤ ලස්සනයි ගයනව සුබපතනව\tNot offensive\n",
            "මම කුබුර හාලා හංදියටගියා විටක් කන්න නංගිකෙනෙක් ඇහුවකියහංකො ඩයලොග්එකේද වැඩකරන්නෙ කයලා.\tNot offensive\n",
            "Sirawatamaaa aaaaaa\tNot offensive\n",
            "Hutch Dial Tunes සක්‍රිය කර රුපියල් මිලියනයක් දිනාගන්න!\tNot offensive\n",
            "ඕනෑම කෙනෙකුට Dial කරන විට ඇසෙන ring ring නාදය වෙනුවට ඔබ කැමතිම සින්දුව අහන්න.\tNot offensive\n",
            "වැඩි විස්තර - www.hutch.lk/dial-tunes\tNot offensive\n",
            "You’re talking about the latest season being aired on HBO on the 14th of this month, right?\tNot offensive\n",
            "I'm currently using Mini SIM (B) A - Full size SIM B - Mini SIM C - Micro SIM D - Nano SIM\tNot offensive\n",
            "ganna epa wadak naa සතියක් තිස්සේ ෆයිබර් සම්බන්ධතාවය වැඩ කරන්නේ නැත.\tNot offensive\n",
            "ටීවී බැලීම, අන්තර්ජාල මගින් රැකියා කිරීම, දුරකථන භාවිතා කිරීම, සියල්ල නැවතී ඇත.\tNot offensive\n",
            "Ow anivaren mathakayi\tNot offensive\n",
            "මා 2020.03.29 වන දින ඩයලොග් වෙබ්සයිට් එක මගින් ඔන්ලයින් රීලෝඩ් ලෙස රුපියල් 500ක මුදලක් ඇනවුම් කළෙමි.\tNot offensive\n",
            "එහිදී මාගේ ලංකා බැංකු ගිණුමෙන් රුපියල් 500ක මුදලක් අය වී තිබුණද මා හට එම රීලෝඩ් මුදල මෙතෙක් ලැබුනේ නැත.\tNot offensive\n",
            "එම අවස්ථාවේදී මා ඩයලොග් ආයතනයේ දුරකථන අංක 1777 ඔස්සේ ඒ පිළිබඳව ඔවුන් දැනුවත් කළෙමි.\tNot offensive\n",
            "එහිදී ඔවුන් පැවසුවේ තව පැයකින් පමණ එම මුදල ලැබිය හැකි බවයි.\tNot offensive\n",
            "තව දුරටත් මේ පිළිබඳව සෙවීමට එම ගණුදෙනුවේ reference number එක බැංකුව මගින් ලබා ගත යුතු බව පවසන ලදි.\tNot offensive\n",
            "එබැවින් ඒ පිළිබඳව සෙවීම සඳහා මා 2020.03.30 දින ලංකා බැංකු ශාකාවට ගොස් එම අංකය ලබා ගතිමි.\tNot offensive\n",
            "එහිදී ඩයලොග් අයතනයට මාගෙ ගිණුමෙන් මුදල් අය වී ඇති බව ඔවුන් පවසන ලදි.\tNot offensive\n",
            "මේ පිළිබඳව එදින උදෑසන ඩයලොග් ආයතනයට ඇමතුමක් ලබා දී දැනුම් දීමේදී ඔවුන් හට මුදල් ලැබී ඇති බවත් එම මුදල පැය 4ක් ඇතුලත ලබා දෙන බවත් පවසන ලදි.\tNot offensive\n",
            "පැය 4ක් ගත වූවත් එම මුදල නිලැබීම හේතුවෙන් නැවත දුරකථන ඇමතුමක් ලබා දුන්නෙමි.\tNot offensive\n",
            "මේ සඳහා විනාඩි 20ක පමන කාලයක් ගත විය.\tNot offensive\n",
            "එහිදීද කිසිම ප්‍රතිචාරයක් නොලැබුනි.\tNot offensive\n",
            "එහිදී එම මුදල බැංකුව මගින් හිරකර තබා ගෙන ඇති බව ඔවුන් පවසයි .\tNot offensive\n",
            "එහෙත් උදෑසන ඇමතුමෙන් සහ බැංකුවට ගොස් නිරීක්ෂණය කිරීමේදී එම මුදල ඩයලොග් ආයතනයට බැර වී ඇති බව දැන්ගන්නට ලැබුණි.\tNot offensive\n",
            "මෙවන් බොරු ගොතා පාරිබෝගිකයන් රවටන්නෙ ඇයි දැයි නොදනිමි.\tNot offensive\n",
            "මෙය මොවුන් සිදුකරන වංචාවක් බව හැඟී යයි.\tNot offensive\n",
            "ප.ව 5ට පෙර ලබා දෙන බව ඉන් පසුව පැවසුවද ඒ වන විටත් මුදල ලැබුනේ නැත.\tNot offensive\n",
            "ඉන් පසුව මා දුරකථන ඇමතුමක් ලබා දීමේදී අප හට කිරීමට දෙයක් නොමැති බවත් අදාල අංශ දැනුවත් කළ බවත් දැනුම් දෙයි.\tNot offensive\n",
            "අදාල අංශ සමග සම්බන්ධකර දෙන ලෙස ඉල්ලීමක් කළද ඔවුන් ඒ සඳහා අවස්ථාවක් ලබා දුන්නේ නැත.\tNot offensive\n",
            "මේ සම්බන්ධව විදුලි සංදේශ නියාමන කොමිසමට පැමිණිල්ලක්ද ඉදිරිපත් කර ඇත.\tNot offensive\n",
            "මට මේ මාසෙ loyalty data හම්බුනේ නෑ යකෝ 😡😡\tAbusive\n",
            "Mge active wenne na magula\tNot offensive\n",
            "ada world xi vs westindies match aka mytv app aken denne\tNot offensive\n",
            "Wow\tNot offensive\n",
            "But no whatsapp\tNot offensive\n",
            "Thopita oya hu..ha karanna barinan oka vahapiyaw\tAbusive\n",
            "ලඟකදි ඉදලා කණුව ඇල්ලෙන්නෙ නෑ යකෝ😂\tAbusive\n",
            "සිග්නල් එනවා යනවා පට්ට ස්පීඩ් ඉන්ටර්නෙට් කියලා වැඩක් නෑ 🖕😂\tNot offensive\n",
            "හෝ ද යි\tNot offensive\n",
            "ඩේටා කන්න එපා බුරැවා\tAbusive\n",
            "Disconnect කරලා තිබුණා dailog Tv line eka full payments කරලා activities කරගත්තා chenal ඔක්කොම දෙනවා කියපු නිසා.\tNot offensive\n",
            "customer care 10 වැඩිය කතා කරලා කිව්වා.\tNot offensive\n",
            "එක එක bole pas කරනවා උත්තර denne නෑ හරියට\tNot offensive\n",
            "මිනිස්සුන්ට බොරු කියලා වැඩ කරන්නේ නැතුව ගු puchan කාපියව් හිඟන්නෝ ඔයිට වඩා නම්බුකාර එක\tAbusive\n",
            "Sprrrrrrrrrrrrrrrrrrrrrrrr\tNot offensive\n",
            "Tnq\tNot offensive\n",
            "මේක කරන්න 5G ඔනේ නෑනේ....\tNot offensive\n",
            "Hutch අපට 4G දෙන්නෙ නැද්ද?3G speed මදිනේ\tNot offensive\n",
            "Very speed.. 😂😂😂😂never can't been until expire date.\tNot offensive\n",
            "Data is over before expire date.. 😂😂😂\tNot offensive\n",
            "Thank you so much SLT.\tNot offensive\n",
            "SLT is the most trusted service....\tNot offensive\n",
            "Watch LIVE action of T20 WORLD CUP 2016-2nd Semifinal match West Indies vs India on Star Sports1 & Star Sport HD1 today from 7.00pm onwards\tNot offensive\n",
            "Plz help inbox\tNot offensive\n",
            "data maaraya 😪\tAbusive\n",
            "Please add Facebook Twitter Youtube to this list., 😇\tNot offensive\n",
            "If u luv us more add Netflix too.\tNot offensive\n",
            "Then we don't go out forever!!! 💓\tNot offensive\n",
            "Congratulations chasbonz naula\tNot offensive\n",
            "Mobitel paludu una sim ekkt aluth ekk gnn puluwn da\tNot offensive\n",
            "Sri Lanka Telecom PLC 0112121212 Extra GB hotline is not working... :(\tNot offensive\n",
            "Rathna Nanayakkara\tNot offensive\n",
            "Your service is worst.\tNot offensive\n",
            "Peo TV and Internet is down from 3 days.\tNot offensive\n",
            "No response yet.\tNot offensive\n",
            "Complained to the 1212 and regional manager in Matara branch.\tNot offensive\n",
            "No response yet.\tNot offensive\n",
            "Niyamai lahiruwa\tNot offensive\n",
            "Hutch 4g apn setting ewanna\tNot offensive\n",
            "මන් ඉයේ රිලොඩ් කරා මහජන බැංකුවේන්,මට ආවේ නැ ෆ්‍රි රීලොඩ් එක,🤨\tNot offensive\n",
            "Free data tikk denna oyala nm wadk na\tNot offensive\n",
            "Night time data\tNot offensive\n",
            "Ha ithin\tNot offensive\n",
            "Where is free data 😷😷\tNot offensive\n",
            "Free data on 1st 🤣\tNot offensive\n",
            "Api edath adath hetat dilog walata adarei...mona unath dilog hodai\tNot offensive\n",
            "Kisima wedak neti sim.\tNot offensive\n",
            "anek jalawala podi hari sahanayak denawa.\tNot offensive\n",
            "munta hena gahala\tAbusive\n",
            "Hutch data sim eke details denko.\tNot offensive\n",
            "price eka ehma\tNot offensive\n",
            "Labba thhama....\tAbusive\n",
            "අනතුරු ඇඟවීමක් - උතුරට ඊලාම් රාජ්‍යක් ඉල්ලුවා වගේ්.. දකුණටත් රාජ්‍යක් ඉල්ලුවොත් SLT උඹලා තමයි වගකියන්න ඕනි...👉🧐🙃😂\tHate-Inducing\n",
            "උන් දැන් පොම් වෙලා ඇති🙉\tAbusive\n",
            "Slt is too much lie\tNot offensive\n",
            "අද කියලා වැඩක් නැහැනේ...\tNot offensive\n",
            "සීදුවට Signal ne 4G wifi\tNot offensive\n",
            "Hooooooo yeeeeeeee\tNot offensive\n",
            "Signal ne ne anee.\tNot offensive\n",
            "Ekata mokak hari karannakoo.\tNot offensive\n",
            "A 50 samsung galaxy\tNot offensive\n",
            "3G 4n walatat me pack use krnna puluwanda\tNot offensive\n",
            "Rampeta pori heli\tNot offensive\n",
            "Ai mehema watenne\tNot offensive\n",
            "Where is this free data ?\tNot offensive\n",
            "4G ko\tNot offensive\n",
            "පිනක් වෙන්න කියලා Data tikak දෙන්නකු\tNot offensive\n",
            "156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zIZboVQohoK"
      },
      "source": [
        "from cassis import *\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = \"/content/drive/Shareddrives/FYP-CodeStars/\"\n",
        "annotation_path = \"/content/drive/Shareddrives/FYP-CodeStars/Annotations/\"\n",
        "\n",
        "dataset_paths_dic = {\"Jayani\": [\"Dataset_1.xmi\"], \"Janani\": [\"Dataset_1.xmi\"], \"Raveesha\": [\"Dataset_1.xmi\", \"Dataset_2.xmi\"], \"Himashi\": [\"Dataset_1.xmi\", \"Dataset_2.xmi\"]}\n",
        "\n",
        "all_data = pd.DataFrame(columns=['id', 'comment', 'label'])\n",
        "\n",
        "count=0\n",
        "\n",
        "for annotator in dataset_paths_dic:\n",
        "  for path in dataset_paths_dic[annotator]:\n",
        "    \n",
        "    with open(annotation_path+annotator+\"/TypeSystem.xml\", 'rb') as f:\n",
        "      typesystem = load_typesystem(f)\n",
        "\n",
        "    with open(annotation_path+annotator+\"/\"+path, 'rb') as f:\n",
        "      doc = load_cas_from_xmi(f, typesystem=typesystem)\n",
        "\n",
        "    for sentence in doc.select('webanno.custom.HateSpeech'):\n",
        "        all_data.loc[count] = [count+1] + [sentence.get_covered_text()] + [sentence.HateSpeech]\n",
        "        count += 1\n",
        "\n",
        "all_data = all_data.replace(\"Hate-Including\", \"Hate-Inducing\")\n",
        "all_data = all_data.replace(\"Not offensivevg\", \"Not offensive\")\n",
        "\n",
        "all_data.to_csv(annotation_path + \"Hate_Speech_Annotation.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujNBmefh8LrN"
      },
      "source": [
        "from cassis import *\n",
        "import pandas as pd\n",
        "\n",
        "annotation_path = \"/content/drive/Shareddrives/FYP-CodeStars/Annotations/Raveesha\"\n",
        "\n",
        "with open(annotation_path+\"/TypeSystem.xml\", 'rb') as f:\n",
        "  typesystem = load_typesystem(f)\n",
        "\n",
        "with open(annotation_path+\"/Full_Kappa_Annotation_Raveesha.xmi\", 'rb') as f:\n",
        "  doc = load_cas_from_xmi(f, typesystem=typesystem)\n",
        "\n",
        "all_data = pd.DataFrame(columns=['id', 'comment', 'label'])\n",
        "count=0\n",
        "\n",
        "for sentence in doc.select('webanno.custom.Humor'):\n",
        "  all_data.loc[count] = [count+1] + [sentence.get_covered_text()] + [sentence.Humor]\n",
        "  count += 1\n",
        "\n",
        "# all_data = all_data.replace(\"Hate-Including\", \"Hate-Inducing\")\n",
        "# all_data = all_data.replace(\"Not offensivevg\", \"Not offensive\")\n",
        "\n",
        "all_data.to_csv(annotation_path + \"/\"+\"Raveesha_Humor_Kappa.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hl0X18vVxEp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}